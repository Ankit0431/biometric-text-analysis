"""
LLM-likeness detector (heuristic-based prototype).

Detects text that appears to be generated by LLMs using statistical heuristics:
- Sentence length variance (LLMs tend to have more uniform sentence lengths)
- Punctuation entropy (LLMs tend to use punctuation more predictably)
- Phrase templates and patterns common in LLM outputs
- Vocabulary diversity metrics

Returns a penalty in [0, 0.2] to apply to the verification score.
"""

import re
import math
from typing import Tuple, Dict, Any
import numpy as np


# Common LLM phrases/patterns (case-insensitive)
LLM_PHRASES = [
    r'\bas an ai\b',
    r'\bi (don\'t|do not) have (personal|the ability)',
    r'\bi (cannot|can\'t) (provide|access|browse)',
    r'\bmy (knowledge|training) (cutoff|was|ends)',
    r'\bi\'m (just|only) an? (ai|language model)',
    r'\bi apologize for (any|the) confusion',
    r'\blet me (clarify|explain|provide)',
    r'\bin summary\b',
    r'\bin conclusion\b',
    r'\bit\'s (important|worth) (noting|mentioning) that',
    r'\bto (answer|address) your question',
    r'\bfeel free to (ask|reach out|contact)',
    r'\bif you (have|need) (any|further|more) (questions|information|help)',
    r'\bhowever\b.*\bit\'s important',
    r'\bplease note that',
]

# Compile patterns for efficiency
LLM_PATTERN_RE = re.compile('|'.join(LLM_PHRASES), re.IGNORECASE)


def detect_llm_likeness(text: str) -> Tuple[float, bool, Dict[str, Any]]:
    """
    Detect if text appears to be LLM-generated.

    Args:
        text: The text to analyze

    Returns:
        Tuple of (penalty, llm_like_bool, details_dict):
        - penalty: float in [0, 0.2] to subtract from verification score
        - llm_like_bool: True if text appears LLM-generated
        - details_dict: Diagnostic information
    """
    # Handle empty or very short text
    if not text or len(text.strip()) < 50:
        return 0.0, False, {"reason": "text_too_short"}

    text = text.strip()

    # Extract features
    features = _extract_features(text)

    # Score based on heuristics
    score = 0.0
    flags = []

    # 1. Sentence length uniformity (LLMs tend to be more uniform)
    sentence_var = features['sentence_length_variance']
    if sentence_var < 100:  # Very uniform sentence lengths
        score += 0.06
        flags.append("uniform_sentences")

    # 2. Punctuation entropy (LLMs use punctuation more predictably)
    punct_entropy = features['punctuation_entropy']
    if punct_entropy < 0.5:  # Low entropy = predictable punctuation
        score += 0.05
        flags.append("predictable_punctuation")

    # 3. LLM phrase patterns
    if features['llm_phrase_count'] > 0:
        score += min(0.08, features['llm_phrase_count'] * 0.03)
        flags.append(f"llm_phrases({features['llm_phrase_count']})")

    # 4. Vocabulary diversity (LLMs sometimes show lower diversity)
    vocab_diversity = features['vocabulary_diversity']
    if vocab_diversity < 0.4:  # Low diversity
        score += 0.04
        flags.append("low_vocabulary_diversity")

    # 5. Average sentence length (LLMs tend toward moderate lengths)
    avg_sent_len = features['avg_sentence_length']
    if 15 <= avg_sent_len <= 25:  # Sweet spot for LLM generation
        score += 0.03
        flags.append("typical_llm_length")

    # 6. Punctuation rate (LLMs tend to punctuate correctly)
    if 0.03 <= features['punctuation_rate'] <= 0.08:
        score += 0.02
        flags.append("typical_punctuation_rate")

    # 7. Check for very polished, error-free text patterns
    if features['contractions_rate'] < 0.01 and features['avg_sentence_length'] > 12:
        score += 0.04
        flags.append("formal_no_contractions")

    # Cap penalty at 0.2
    penalty = min(0.2, score)

    # Threshold for "LLM-like" classification
    llm_like = penalty >= 0.10

    details = {
        "penalty": penalty,
        "llm_like": llm_like,
        "flags": flags,
        "features": features,
    }

    return penalty, llm_like, details


def _extract_features(text: str) -> Dict[str, float]:
    """Extract statistical features from text."""

    # Split into sentences (simple heuristic)
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return {
            'sentence_length_variance': 0,
            'punctuation_entropy': 0,
            'llm_phrase_count': 0,
            'vocabulary_diversity': 0,
            'avg_sentence_length': 0,
            'punctuation_rate': 0,
            'contractions_rate': 0,
        }

    # Sentence length statistics
    sent_lengths = [len(s.split()) for s in sentences]
    avg_sent_len = np.mean(sent_lengths)
    sent_variance = np.var(sent_lengths) if len(sent_lengths) > 1 else 0

    # Punctuation analysis
    punctuation = '.!?,;:\'"()-'
    punct_chars = [c for c in text if c in punctuation]
    punct_counts = {}
    for p in punct_chars:
        punct_counts[p] = punct_counts.get(p, 0) + 1

    # Calculate punctuation entropy
    punct_entropy = 0.0
    if punct_chars:
        total = len(punct_chars)
        for count in punct_counts.values():
            p = count / total
            punct_entropy -= p * math.log2(p)

    # Punctuation rate (per 100 chars)
    punct_rate = len(punct_chars) / len(text) if text else 0

    # LLM phrase detection
    llm_matches = LLM_PATTERN_RE.findall(text)
    llm_phrase_count = len(llm_matches)

    # Vocabulary diversity (unique words / total words)
    words = re.findall(r'\b\w+\b', text.lower())
    vocab_diversity = len(set(words)) / len(words) if words else 0

    # Contractions rate
    contractions = re.findall(r"\b\w+'\w+\b", text)
    contractions_rate = len(contractions) / len(words) if words else 0

    return {
        'sentence_length_variance': float(sent_variance),
        'punctuation_entropy': float(punct_entropy),
        'llm_phrase_count': llm_phrase_count,
        'vocabulary_diversity': float(vocab_diversity),
        'avg_sentence_length': float(avg_sent_len),
        'punctuation_rate': float(punct_rate),
        'contractions_rate': float(contractions_rate),
    }


def get_penalty(text: str) -> float:
    """
    Convenience function to get just the penalty value.

    Args:
        text: The text to analyze

    Returns:
        Penalty value in [0, 0.2]
    """
    penalty, _, _ = detect_llm_likeness(text)
    return penalty


def is_llm_generated(text: str, threshold: float = 0.10) -> bool:
    """
    Check if text appears to be LLM-generated.

    Args:
        text: The text to analyze
        threshold: Penalty threshold for classification (default 0.10)

    Returns:
        True if penalty >= threshold
    """
    penalty, _, _ = detect_llm_likeness(text)
    return penalty >= threshold
